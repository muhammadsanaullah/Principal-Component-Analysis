# Principal-Component-Analysis
Implementation of PCA in MATLAB to reduce multidimensional data to fewer dimensions for easier and more efficient data analysis.

In most neuroscientific experiments, the data obtained is multidimensional and occupies a lot of disk space for computational analysis. A proportion of this data is actually not as relevant as the other proportion of the data, which puts forth an intuitive idea to reduce the dimensionality of data for more efficient processing.
Principal Components Analysis (PCA) performs a linear transformation on data and can be used to reduce multidimensional data down to a few dimensions for easier analysis. The idea is that many large datasets contain correlations between the dimensions, so that a portion of the data is redundant. PCA will transform the data so that as much variation as possible will be crammed into the fewest possible dimensions. This allows us to compress the data by ignoring other dimensions.

### A Neuroscientific Application of PCA
One common application of PCA is the spike sorting of neural data. Typically, a data
acquisition system monitors a raw voltage trace. Every time the voltage crosses some
threshold, the raw voltage is sampled during a time window surrounding this crossing to
produce the recorded spike waveform. For this experiment we will build our own primitive spike sorter using PCA to analyze extracellular data from recordings in the primary motor cortex of a nonhuman primate (data courtesy of the Hatsopoulos laboratory).
We have data from sessions from 2 different days. Each day’s session contains extracellular recordings modelled as spikes. These range from 80,000 – 120,000, with each waveform containing 48 points; a total of 6 million sample points. Using PCA, we reduce the dimensionality by considering each waveform as a single point, and each data point as a dimension, which gives us a 48-dimensional space. PCA is essentially just a coordinate transformation. The original data are plotted on an X-axis and a Y-axis. PCA seeks to rotate these two axes so that the new axis X’ lies along the direction of maximum variation in the data. PCA requires that the axes be perpendicular, so in two dimensions the choice of X’ will determine Y’. You obtain the transformed data by reading the x and y values off this new set of axes, X’ and Y’. For more than two dimensions, the first axis is in the direction of most variation; the second, in direction of the next-most variation; and so on.
From PCA, we obtain eigenvalues which provide information of amount of variance of data along a certain principal component, and eigenvectors form those principal components on which the data is projected. So, the data is first modelled onto a covariance matrix, from which eigenvalues and eigenvectors are found. The greatest eigenvectors are picked for the greatest eigenvalues, these are the dimensions along which the data varies the most i.e., it’s important to include this data. The data is projected along those eigenvectors, which act as our principal components. We can then test for our PCA modelled data by using RMS errors between original and projected data, as well as using an analysis of data retention as per different number of principal components.
